{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cac7a57-cd58-45d9-b12f-46614fa5e7ae",
   "metadata": {},
   "source": [
    "### 1. 预处理\n",
    "#### 1.1. JSON转为TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca8dc5b5-e321-41c2-a907-d4d4eee51dbc",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df20c2a8-a9f4-4eb5-9224-01fb84bebe36",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def json2tsv(input_file = \"./NER/IMCS-V2_dev.json\",output_file = \"./NER/dev.tsv\"):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as src,open(output_file,\"w\", encoding=\"utf-8\") as dst:\n",
    "        print(\"dialogue_id\\tsent_id\\ttext\\tlabel\\n\",file=dst)\n",
    "        # 使用json.load将文件内容解析为Python对象\n",
    "        data = json.load(src)\n",
    "        for key,value in data.items():\n",
    "            dialogue_id = key\n",
    "            dialogue = value[\"dialogue\"]\n",
    "            for sentence in dialogue:\n",
    "                sent_id = sentence[\"sentence_id\"]\n",
    "                text = sentence[\"sentence\"]\n",
    "                label = sentence[\"BIO_label\"]\n",
    "                print(f\"{dialogue_id}\\t{sent_id}\\t{text}\\t{label}\\n\",file=dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "26e525cb-0b8c-45e4-972a-94de80aea322",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json_train = \"./NER/IMCS-V2_train.json\"\n",
    "tsv_train =  \"./NER/IMCS-V2_train.tsv\"\n",
    "json2tsv(input_file=json_train,output_file=tsv_train)\n",
    "\n",
    "json_dev = \"./NER/IMCS-V2_dev.json\"\n",
    "tsv_dev = \"./NER/IMCS-V2_dev.tsv\"\n",
    "json2tsv(input_file=json_dev,output_file=tsv_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae285af9-a494-4780-9610-593cffa66071",
   "metadata": {},
   "source": [
    "#### 1.2. 使用Seqeval进行评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "383dd662-81f3-4585-957e-8d7837e47679",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "def get_metrics(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    单独计算 Precision, Recall, F1-score\n",
    "    \"\"\"\n",
    "    copy_pred = y_pred.copy()\n",
    "    if len(y_true) != len(copy_pred):\n",
    "        raise ValueError(\"y_true is not the same size with y_pred \")\n",
    "    problematic = 0\n",
    "    for i in range(len(y_true)):\n",
    "        pred = copy_pred[i]\n",
    "        label = y_true[i]\n",
    "        if len(pred) != len(label):\n",
    "            # print(f\"not the same length,\\n pred length: {len(pred)},text = {pred}\\n label length: {len(label)},text={label}\\n\")\n",
    "            # pred = [\"X\"]*len(label)\n",
    "            # print(pred,[\"O\"]*len(label))\n",
    "            pred = (pred + [\"O\"]*len(label))[0:len(label)]\n",
    "            # print(pred)\n",
    "            copy_pred[i] = pred\n",
    "            problematic += 1\n",
    "\n",
    "    precision = precision_score(y_true, copy_pred)\n",
    "    accuracy = accuracy_score(y_true, copy_pred,)\n",
    "    recall = recall_score(y_true, copy_pred)\n",
    "    f1 = f1_score(y_true, copy_pred)\n",
    "\n",
    "    print(f\"# Precision: {precision}\")\n",
    "    print(f\"# Accuracy: {accuracy}\")\n",
    "    print(f\"# Recall: {recall}\")\n",
    "    print(f\"# F1 Score: {f1}\")\n",
    "\n",
    "    # 使用 strict 模式计算 F1-score\n",
    "    f1_strict = f1_score(y_true, copy_pred, mode='strict',average=\"micro\")\n",
    "    print(f\"# F1-micro Score (Strict): {f1_strict}\")\n",
    "    print(f\"# problematic records: {problematic}\")\n",
    "\n",
    "    # 使用 partial 模式计算 F1-score\n",
    "    # f1_partial = f1_score(y_true, y_pred, mode='partial',average=\"micro\")\n",
    "    # print(f\"F1 Score (Partial): {f1_partial}\")\n",
    "    return precision,accuracy,recall,f1,f1_strict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f68cbfc-5e49-4c02-bf3a-c550c3f0be54",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_acc(y_pred,y_true):\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"y_true is not the same size with y_pred \")\n",
    "    results = []\n",
    "    for i in range(len(y_true)):\n",
    "        pred = \" \".join(y_pred[i])\n",
    "        label = \" \".join(y_true[i])\n",
    "        results.append(pred == label)\n",
    "    return sum(results)/len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c0eb85d-019c-4fcb-a1b4-3ccdc0dca72c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "# Precision: 0.6666666666666666\n",
      "# Accuracy: 0.9\n",
      "# Recall: 0.6666666666666666\n",
      "# F1 Score: 0.6666666666666666\n",
      "# F1-micro Score (Strict): 0.6666666666666666\n",
      "# problematic records: 0\n",
      "(0.6666666666666666, 0.9, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666)\n",
      "********************\n",
      "0.5\n",
      "********************\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Drug       0.00      0.00      0.00         1\n",
      "     Symptom       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       0.67      0.67      0.67         3\n",
      "   macro avg       0.50      0.50      0.50         3\n",
      "weighted avg       0.67      0.67      0.67         3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 示例标签\n",
    "y_true = [\n",
    "    [\"O\", \"B-Symptom\", \"I-Symptom\", \"O\", \"B-Drug\", \"I-Drug\"],\n",
    "    [\"O\", \"B-Symptom\", \"I-Symptom\", \"O\"]\n",
    "]\n",
    "y_pred = [\n",
    "    [\"O\", \"B-Symptom\", \"I-Symptom\", \"O\", \"B-Drug\", \"O\"],\n",
    "    [\"O\", \"B-Symptom\", \"I-Symptom\", \"O\"]\n",
    "]\n",
    "print(\"*\"*20)\n",
    "print(get_metrics(y_pred=y_pred,y_true=y_true))\n",
    "print(\"*\"*20)\n",
    "print(get_acc(y_pred=y_pred,y_true=y_true))\n",
    "print(\"*\"*20)\n",
    "# 打印分类报告\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d6258b-d8c1-43db-82a7-98fa070ff6a6",
   "metadata": {},
   "source": [
    "### 2. 使用原始预训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ab8d5-b2e1-40cf-a44b-d43dc86df887",
   "metadata": {},
   "source": [
    "#### 2.1. 加载原始模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83b3b033-0d51-4907-9408-8debacb17d79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1edd4404-4ce1-4334-a8da-344b035a69f7",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using config file: /etc/orion/env/env.conf\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/gemini/pretrain/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be5ff6-3b1f-4256-a03f-9b21c3810bd6",
   "metadata": {},
   "source": [
    "#### 2.2. 使用Prompt和预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "926446c2-e70d-41ab-a56c-f57cf67191c0",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bio_lable(text):\n",
    "    \"\"\"\n",
    "        调用大模型进行NER\n",
    "    \"\"\"\n",
    "    sys_prompt = \"\"\"\n",
    "    你是一个医疗领域的命名实体识别专家，请将用户的输入中的命名实体识别出来并以严格的BIO规范标注，即B-X开头，以I-X结尾，\n",
    "    其中X代表命名实体类别Symptom，Drug，Drug_Category，Medical_Examination，Operation中的一种，O代表不属于任何一个类别，\n",
    "    命名实体的类型包括：\n",
    "    Symptom，病人因患病而表现出来的异常状况，如 发热、呼吸困难、鼻塞 等。\n",
    "    Drug，具体的药物名称，如 妈咪爱、蒙脱石散、蒲地蓝 等。\n",
    "    Drug_Category，根据药物功能进行划分的药物种类，如 消炎药、感冒药、益生菌 等。\n",
    "    Medical_Examination，医学检验，如 血常规、x光片、CRP分析 等。\n",
    "    Operation，相关的医疗操作，如 输液、雾化、接种疫苗 等。\n",
    "    比如用户输入为： “你好，咳嗽是连声咳吗？有痰吗？有没流鼻涕，鼻塞？”，\n",
    "    你的输出为：\"O O O B-Symptom I-Symptom O O O B-Symptom O O O B-Symptom O O O O B-Symptom I-Symptom I-Symptom O B-Symptom I-Symptom O\".\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "        top_p = 0.3,\n",
    "        temperature = 0.1\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response\n",
    "\n",
    "def get_batch_label(texts,labels):\n",
    "    results = []\n",
    "    with open(\"./pred_resultv3_1.tsv\",\"w\", encoding=\"utf-8\") as pred_result:\n",
    "        for i in tqdm(range(len(texts))):\n",
    "            text = texts[i]\n",
    "            label = labels[i]\n",
    "            result = get_bio_lable(text = text).replace(\" <END>\", \"\")\n",
    "            print(f\"{i}\\t{text}\\t{label}\\t{result}\\t{' '.join(label)==result}\",file=pred_result)\n",
    "            result = result.split(\" \")\n",
    "            # print(f\"================={i}=======================\")\n",
    "            # print(f\"input ({len(text)}):\", text)\n",
    "            # print(f\"output({len(result)}):\", result)\n",
    "            # print(f\"label ({len(label)}):\", label)\n",
    "            # if len(label) != len(result):\n",
    "            #     print(\"===WARNING: length unmatched ====\")\n",
    "            results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b54880c-7ed7-48f0-be47-56b4c712d792",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get_bio_lable(\"咳嗽有几天了？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dcb159-e8ad-4934-acd7-b3c46206f324",
   "metadata": {},
   "source": [
    "#### 2.3. 获取训练与开发数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57d9c95c-47d1-422f-a031-a4ca2e2d2879",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98529,) (98529,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv(tsv_train,sep=\"\\t\",encoding=\"utf-8\");\n",
    "X_train = train_data[\"text\"].to_numpy()\n",
    "y_train = train_data[\"label\"].apply(lambda x: x.split(\" \")).to_numpy()\n",
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d2c0084-0082-4c7f-bd8b-f2b1e979eb9d",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33267,) (33267,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dev_data = pd.read_csv(tsv_dev,sep=\"\\t\",encoding=\"utf-8\")\n",
    "# .sample(n=10)\n",
    "X_dev = dev_data[\"text\"].to_numpy()\n",
    "y_dev = dev_data[\"label\"].apply(lambda x: x.split(\" \")).to_numpy()\n",
    "print(X_dev.shape,y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6889b85d-ec55-489a-ab3f-95c3910fd180",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('大便怎么样？干不干？胃口怎么样？', 'O O O O O O O O O O O O O O O O')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 8\n",
    "X_dev[n],\" \".join(y_dev[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6077686c-e668-4f97-909a-199b7a760c68",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "not the same length,\n",
      " pred length: 14,text = ['O', 'B-Symptom', 'I-Symptom', 'I-Symptom', 'O', 'B-Symptom', 'I-Symptom', 'O', 'B-Symptom', 'I-Symptom', 'O', 'O', 'O', 'O']\n",
      " label length: 13,text=['O', 'B-Symptom', 'I-Symptom', 'O', 'B-Symptom', 'I-Symptom', 'O', 'B-Symptom', 'I-Symptom', 'O', 'O', 'O', 'O']\n",
      "\n",
      "['O', 'B-Symptom', 'I-Symptom', 'I-Symptom', 'O', 'B-Symptom', 'I-Symptom', 'O', 'B-Symptom', 'I-Symptom', 'O', 'O', 'O', 'O'] ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'B-Symptom', 'I-Symptom', 'I-Symptom', 'O', 'B-Symptom', 'I-Symptom', 'O', 'B-Symptom', 'I-Symptom', 'O', 'O', 'O']\n",
      "not the same length,\n",
      " pred length: 15,text = ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Symptom', 'I-Symptom']\n",
      " label length: 40,text=['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Symptom', 'I-Symptom', 'O', 'B-Operation', 'I-Operation', 'I-Operation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Symptom', 'I-Symptom'] ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Symptom', 'I-Symptom', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "not the same length,\n",
      " pred length: 15,text = ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Symptom', 'I-Symptom', 'I-Symptom']\n",
      " label length: 499,text=['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Symptom', 'I-Symptom', 'I-Symptom', 'O', 'B-Operation', 'I-Operation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Symptom', 'I-Symptom', 'I-Symptom'] ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Symptom', 'I-Symptom', 'I-Symptom', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "# Precision: 0.3333333333333333\n",
      "# Accuracy: 0.9743589743589743\n",
      "# Recall: 0.25\n",
      "# F1 Score: 0.28571428571428575\n",
      "# F1-micro Score (Strict): 0.28571428571428575\n",
      "# problematic records: 3\n",
      "(0.3333333333333333, 0.9743589743589743, 0.25, 0.28571428571428575, 0.28571428571428575)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 练前的测试，0.5B和7B的准确率都接近0\n",
    "y_pred = get_batch_label(X_dev,y_dev)\n",
    "print( get_acc( y_pred, y_dev ) )\n",
    "print( get_metrics( y_pred, y_dev ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83897467-6ca8-4020-8f7b-b0d63c7a8513",
   "metadata": {},
   "source": [
    "### 3. 微调准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa461e-c0c7-4e6c-ac54-52bbd78a929c",
   "metadata": {},
   "source": [
    "#### 3.1 准备训练数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70c99a71-d132-4bcf-a871-8b1077192e8c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def json2Alpaca(src,dst):\n",
    "    sys_prompt = \"请将用户的输入中的命名实体识别出来并以严格的BIO规范标注输出\"\n",
    "        # \"\"\"\n",
    "        # 请将用户的输入中的命名实体识别出来并以严格的BIO规范标注，即B-X开头，以I-X结尾，\n",
    "        # 其中X代表命名实体类别Symptom，Drug，Drug_Category，Medical_Examination，Operation中的一种，O代表不属于任何一个类别，\n",
    "        # 命名实体的类型包括：\n",
    "        # Symptom:病人因患病而表现出来的异常状况，如 发热、呼吸困难、鼻塞 等。\n",
    "        # Drug:具体的药物名称，如 妈咪爱、蒙脱石散、蒲地蓝 等。\n",
    "        # Drug_Category:根据药物功能进行划分的药物种类，如 消炎药、感冒药、益生菌 等。\n",
    "        # Medical_Examination:医学检验，如 血常规、x光片、CRP分析 等。\n",
    "        # Operation:相关的医疗操作，如 输液、雾化、接种疫苗 等。\n",
    "        # 比如用户输入为： \"你好，咳嗽是连声咳吗？有痰吗？有没流鼻涕，鼻塞？\"，\n",
    "        # 你的输出为：\"O O O B-Symptom I-Symptom O O O B-Symptom O O O B-Symptom O O O O B-Symptom I-Symptom I-Symptom O B-Symptom I-Symptom O\"\n",
    "        # \"\"\"\n",
    "    entries = []\n",
    "\n",
    "    with open(src, \"r\", encoding=\"utf-8\") as src,open(dst,\"w\", encoding=\"utf-8\") as dst:\n",
    "        # 使用json.load将文件内容解析为Python对象\n",
    "        data = json.load(src)\n",
    "        for key,value in data.items():\n",
    "            # dialogue_id = key\n",
    "            dialogue = value[\"dialogue\"]\n",
    "            for sentence in dialogue:\n",
    "                # sent_id = sentence[\"sentence_id\"]\n",
    "                text = sentence[\"sentence\"]\n",
    "                label = sentence[\"BIO_label\"]\n",
    "\n",
    "                entry = {\"instruction\": sys_prompt,\n",
    "                         \"input\": text,\n",
    "                         \"output\": label,\n",
    "                         \"system\": \"你是一个医疗领域的命名实体识别(NER)专家。\"\n",
    "                        }\n",
    "                # print(entry)  # 打印每一行的数据\n",
    "                entries.append(entry)\n",
    "        json.dump(obj=entries, ensure_ascii=False, indent=4, fp=dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3795971b-3abd-4739-8b39-d35784077588",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json2Alpaca(src=json_train, dst=\"./NER/IMCS-V2_train_alpaca.json\")\n",
    "json2Alpaca(src=json_dev, dst=\"./NER/IMCS-V2_dev_alpaca.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f587cb11-c86a-4218-99b0-2ceac4191ca6",
   "metadata": {},
   "source": [
    "#### 3.2. 使用训练过的0.5B模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f9853fd5-274f-43bc-934f-c6990951129b",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using config file: /etc/orion/env/env.conf\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_name = \"./Qwen2.5-0.5B-Instruct-jinm\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bf921b1d-8983-421b-a52b-79b3abf70a47",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bio_lable2(text):\n",
    "    \"\"\"\n",
    "        调用大模型进行NER\n",
    "    \"\"\"\n",
    "    sys_prompt = \"你是一个医疗命名实体专家！请根据当前对话文本内容，识别出每一句话中的BIO实体标签\"\n",
    "    # \"\"\"\n",
    "    # 你是一个医疗领域的命名实体识别专家，请将用户的输入中的命名实体识别出来并以严格的BIO规范标注，即B-X开头，以I-X结尾，\n",
    "    # 其中X代表命名实体类别Symptom，Drug，Drug_Category，Medical_Examination，Operation中的一种，O代表不属于任何一个类别，\n",
    "    # \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "        top_p = 0.3,\n",
    "        temperature = 0.1\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response\n",
    "\n",
    "def get_batch_label2(texts,labels):\n",
    "    results = []\n",
    "    with open(\"./pred_resultv3_2.tsv\",\"w\", encoding=\"utf-8\") as pred_result:\n",
    "        for i in tqdm(range(len(texts))):\n",
    "            text = texts[i]\n",
    "            label = labels[i]\n",
    "            result = get_bio_lable2(text = text).replace(\" <END>\", \"\")\n",
    "            print(f\"{i}\\t{text}\\t{label}\\t{result}\\t{' '.join(label)==result}\",file=pred_result)\n",
    "            result = result.split(\" \")\n",
    "            # print(f\"================={i}=======================\")\n",
    "            # print(f\"input ({len(text)}):\", text)\n",
    "            # print(f\"output({len(result)}):\", result)\n",
    "            # print(f\"label ({len(label)}):\", label)\n",
    "            # if len(label) != len(result):\n",
    "            #     print(\"===WARNING: length unmatched ====\")\n",
    "            \n",
    "            results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f2bcbe2-7648-4b01-927b-e78096de0b43",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get_bio_lable2(\"咳嗽有几天了？喉咙有痰吗？大便稀？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "40d71d3b-3ad9-4f67-853b-1f4e9064328a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 26073/33267 [3:43:04<51:45,  2.32it/s]   IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 33267/33267 [5:02:23<00:00,  1.83it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8868849009528963\n",
      "# Precision: 0.867416207042851\n",
      "# Accuracy: 0.9639835317681095\n",
      "# Recall: 0.8403551317357886\n",
      "# F1 Score: 0.8536712666235203\n",
      "# F1-micro Score (Strict): 0.8536712666235203\n",
      "# problematic records: 734\n",
      "(0.867416207042851, 0.9639835317681095, 0.8403551317357886, 0.8536712666235203, 0.8536712666235203)\n"
     ]
    }
   ],
   "source": [
    "y_pred = get_batch_label2(X_dev,y_dev)\n",
    "print( get_acc( y_pred, y_dev ) )\n",
    "print( get_metrics( y_pred, y_dev ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32be39d-d6f4-408d-9655-33a95ae4b5e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
